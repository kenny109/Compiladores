import ply.lex as lex

# List of token names
tokens = (
  
    'ID',
    'NUMBER',
    'FLOAT',
    'OP_PLUS',
    'OP_MINUS',
    'OP_TIMES',
    'OP_DIVIDE',
    'LPAREN',
    'RPAREN',
    'SEMICOLON',
    'COLON',
    'OP_EQUAL',
    'OP_AND',
    'OP_OR',
    'L_LEFT',
    'L_RIGHT',
    'TYPE_FLOAT',
    'TYPE_BOOL',
    'TYPE_INT',
    'TYPE_STRING',
    'TYPE_DOUBLE',
    'WHILE',
    'FOR',
    'IF',
    'ELSE',
    'READ',
    'PRINT',
    'COMMENT',
    'LIT',
    'BOOLEAN',
    'FUNCTIONS',
    'OP_LESS_THAN',
    'OP_GREATER_THAN',
    'OP_EQUAL_EQUAL',
    'OP_NOT_EQUAL',
    'RETURN',
   'NEWLINE' 
  
)

# Regular expression rules for simple tokens
t_OP_PLUS    = r'\+'
t_OP_MINUS   = r'-'
t_OP_TIMES   = r'\*'
t_OP_DIVIDE  = r'/'
t_LPAREN     = r'\('
t_RPAREN     = r'\)'
t_SEMICOLON  = r';'
t_COLON      = r':'
t_OP_EQUAL   = r'='
t_OP_AND     = r'and'
t_OP_OR      = r'or'
t_L_LEFT     = r'{'
t_L_RIGHT    = r'}'
t_OP_LESS_THAN  = r'<'
t_OP_GREATER_THAN  = r'>'
t_OP_EQUAL_EQUAL = r'=='
t_OP_NOT_EQUAL   = r'!='

# Define a rule for IDs
def t_FUNCTIONS(t):
  r'function'
  return t
def t_RETURN(t):
  r'return'
  return t
# Define a rule for 'READ'
def t_READ(t):
    r'getc'
    return t

# Define a rule for 'PRINT'
def t_PRINT(t):
    r'showc'
    return t
def t_ID(t):
    r'[a-zA-Z][a-zA-Z0-9_]*'
    return t

# Define a rule for integers
def t_NUMBER(t):
    r'\d+'
    t.value = int(t.value)
    return t

# Define a rule for floating-point numbers
def t_FLOAT(t):
    r'\d+\.\d+'
    t.value = float(t.value)
    return t

# Define a rule for boolean literals
def t_BOOLEAN(t):
    r'true|false'
    return t

# Define a rule for comments
def t_COMMENT(t):
    r'\#\#.*'
    return

# Define a rule for literals
def t_LIT(t):
    r'\".*\"'
    return t



# Define a rule for newline
def t_NEWLINE(t):
    r'\n+'
    t.lexer.lineno += len(t.value)
# Define a rule for ignoring spaces and tabs
t_ignore = ' \t'

# Error handling rule
def t_error(t):
    print(f"Illegal character '{t.value[0]}'")
    t.lexer.skip(1)

# Build the lexer
lexer = lex.lex()

# Open the file and read its contents
with open("tokens.txt", "r") as file:
    data = file.read()

# Give the lexer the input data
lexer.input(data)

# Tokenize
while True:
    tok = lexer.token()
    if not tok: 
        break      # No more input
    print(tok)
